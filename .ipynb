%matplotlib inline

from __future__ import print_function

import numpy as np
import scipy as sp
import pandas as pd
# from tweedie import tweedie
import seaborn as sns
import statsmodels.api as sm
from sklearn import tree
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score,roc_curve,classification_report,confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
import statsmodels.api as sm

df = pd.read_csv('driving.csv')
display(df)

# gradio 
l1 =['VehAge',  'DrivAge' , 'Location_DAMMAM', 'Location_JEDDAH', 'Location_RYIADH']
# the input space : 
l2 =[0,22,False,True,False]
df2 = pd.DataFrame(data = {l1[i]: [l2[i]] for i in range(len(l1))})
display(df2)




df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df = df.rename(columns={'Claim Date': 'AccDate',
                        'Car Brand': 'CarBrand',
                        'Claim Amt2': 'ClaimAmt'})
df = df.iloc[: , :-1]


df 



# Features, One hot Encoding


columns_to_remove = ['ClaimAmt', 'ID', 'AccDate','CarBrand']
xfeat = df.drop(columns=columns_to_remove, axis=1)


xfeat = pd.get_dummies(xfeat, columns=['Location'])
xfar=np.asarray(xfeat)
xfeat
df2




gbm = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)
#ytar = ytar.ravel()
gbm.fit(xfeat, ytar)
#df2 = df.drop(columns=columns_to_remove, axis=1)

#prediction = gbm.predict(xfeat)
gbm.predict(df2)
#pd.DataFrame(prediction)
#z = gbm.predict(xfeat)
